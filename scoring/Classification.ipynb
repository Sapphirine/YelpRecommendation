{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECBM E4040 - Assignment 2- Task 5: Kaggle Open-ended Competition\n",
    "\n",
    "Kaggle is a platform for predictive modelling and analytics competitions in which companies and researchers post data and statisticians and data miners compete to produce the best models for predicting and describing the data.\n",
    "\n",
    "If you don't have a Kaggle account, feel free to join at [www.kaggle.com](https://www.kaggle.com). To let the TAs do the grading more conveniently, please use Lionmail to join Kaggle and use UNI as your username.\n",
    "\n",
    "Visit the website for this competition to join: \n",
    "[https://www.kaggle.com/t/8dd419892b1c49a3afb0cea385a7e677](https://www.kaggle.com/t/8dd419892b1c49a3afb0cea385a7e677)\n",
    "\n",
    "Details about this in-class competition is shown on the website above. Please read carefully.\n",
    "\n",
    "<span style=\"color:red\">__TODO__:</span>\n",
    "1. Train a custom model for the bottle dataset classification problem. You are free to use any methods taught in the class or found by yourself on the Internet (ALWAYS provide reference to the source). General training methods include:\n",
    "    * Dropout\n",
    "    * Batch normalization\n",
    "    * Early stopping\n",
    "    * l1-norm & l2-norm penalization\n",
    "2. You'll be given the test set to generate your predictions (70% public + 30% private, but you don't know which ones are public/private). Achieve 70% accuracy on the public test set. The accuracy will be shown on the public leaderboard once you submit your prediction .csv file. \n",
    "3. (A) Report your results on the Kaggle, for comparison with other students' optimization results (you should do this several times). (C) Save your best model, using BitBucket, at the same time when you (B) submit the homework files into Courseworks. See instructions below. \n",
    "\n",
    "__Hint__: You can start from what you implemented in task 4. Another classic classification model named 'VGG16' can also be easily implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW Submission Details:\n",
    "There are three components to reporting the results of this task: \n",
    "\n",
    "**(A) Submission (possible several) of the .csv prediction file throught the Kaggle platform;**. You should start doing this VARY early, so that students can compare their work as they are making progress with model optimization.\n",
    "\n",
    "**(B) Editing and submitting the content of this Jupyter notebook, through Courseworks; **\n",
    "(i) The code for your CNN model and for the training function. The code should be stored in __./ecbm4040/neuralnets/kaggle.py__;\n",
    "(ii) Print out your training process and accuracy __within this notebook__;\n",
    "\n",
    "**(C) Submitting your best CNN model through instructor-owned private BitBucket repo.**\n",
    "\n",
    "**Description of (C):** \n",
    "For this task, you will be utilizing bitbucket to save your model for submission. Bitbucket provides Git code managment. For those who are not familiar with git operations, please check [Learn Git with Bitbucket Cloud](https://www.atlassian.com/git/tutorials/learn-git-with-bitbucket-cloud) as reference.\n",
    "**TAs will create a private Bitbucket repository for each student, with the write access. This repo will be owned by the instructors. Make sure to properly submit your model to that exact repository (submissions to your own private repository will not count)** Students need to populate the following file to provide instructors with bitbucket account information: https://docs.google.com/spreadsheets/d/1_7cZjyr34I2y-AD_0N5UaJ3ZnqdhYcvrdoTsYvOSd-g/edit#gid=0.\n",
    "\n",
    "<span style=\"color:red\">__Submission content:__ :</span>\n",
    "(i) Upload your best model with all the data output (for example, __MODEL.data-00000-of-00001, MODEL.meta, MODEL.index__) into the  BitBucket. Store your model in the folder named \"__KaggleModel__\" within the BitBucket repository. \n",
    "Remember to delete any intermediate results, **we only want your best model. Do not upload any data files**. The instructors will rerun the uploaded best model and verify against the score which you reported on the Kaggle.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from classification1 import load_data as load\n",
    "from matplotlib import pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=1\n",
    "for i in *.jpg; do\n",
    "  new=$(printf \"%04d.jpg\" \"$a\") #04 pad to length of 4\n",
    "  mv -i -- \"$i\" \"$new\"\n",
    "  let a=a+1\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os, sys\n",
    "\n",
    "path = \"/Users/mikezhao/Downloads/Bigdata/classification/5/\"\n",
    "dirs = os.listdir( path )\n",
    "\n",
    "def resize():\n",
    "    for item in dirs:\n",
    "        if os.path.isfile(path+item):\n",
    "            Image.open(path+item).convert('RGB').save(path+item)\n",
    "            im = Image.open(path+item)\n",
    "            f, e = os.path.splitext(path+item)\n",
    "            imResize = im.resize((256,256), Image.ANTIALIAS)\n",
    "            imResize.save(f + '.jpg', 'JPEG', quality=90)\n",
    "\n",
    "resize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"/Users/mikezhao/Downloads/Bigdata/trainbad/\"\n",
    "dirs = os.listdir( path )\n",
    "\n",
    "def resize():\n",
    "    for item in dirs:\n",
    "        if os.path.isfile(path+item):\n",
    "            Image.open(path+item).convert('RGB').save(path+item)\n",
    "            im = Image.open(path+item)\n",
    "            f, e = os.path.splitext(path+item)\n",
    "            imResize = im.resize((256,256), Image.ANTIALIAS)\n",
    "            imResize.save(f + '.jpg', 'JPEG', quality=90)\n",
    "\n",
    "resize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6586, 256, 256, 3)\n",
      "(6586,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = load(mode='all')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(586, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "num_training = 6000\n",
    "num_validation = 586\n",
    "arr = np.arange(6586)\n",
    "np.random.shuffle(arr)\n",
    "X_val = X_train[arr[0:num_validation]]\n",
    "y_val = y_train[arr[0:num_validation]]\n",
    "X_train = X_train[arr[num_validation:]]\n",
    "y_train = y_train[arr[num_validation:]]\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building my LeNet. Parameters: \n",
      "conv_featmap=[32, 64]\n",
      "fc_units=[84]\n",
      "conv_kernel_size=[5, 5]\n",
      "pooling_size=[2, 2]\n",
      "l2_norm=0.01\n",
      "seed=2\n",
      "learning_rate=0.01\n",
      "number of batches for training: 46\n",
      "epoch 1 \n",
      "epoch 2 \n",
      "Best validation accuracy! iteration:50 accuracy: 50.51194539249147%\n",
      "epoch 3 \n",
      "Best validation accuracy! iteration:100 accuracy: 64.33447098976109%\n",
      "epoch 4 \n",
      "Best validation accuracy! iteration:150 accuracy: 64.84641638225256%\n",
      "epoch 5 \n",
      "epoch 7 \n",
      "Best validation accuracy! iteration:300 accuracy: 69.62457337883959%\n",
      "epoch 8 \n",
      "Best validation accuracy! iteration:350 accuracy: 69.79522184300342%\n",
      "epoch 9 \n",
      "Best validation accuracy! iteration:400 accuracy: 69.96587030716724%\n",
      "epoch 10 \n",
      "epoch 11 \n",
      "Best validation accuracy! iteration:500 accuracy: 72.0136518771331%\n",
      "epoch 12 \n",
      "epoch 13 \n",
      "epoch 14 \n",
      "Best validation accuracy! iteration:600 accuracy: 74.06143344709898%\n",
      "epoch 15 \n",
      "epoch 16 \n",
      "epoch 17 \n",
      "epoch 18 \n",
      "Best validation accuracy! iteration:800 accuracy: 75.93856655290102%\n",
      "epoch 19 \n",
      "Best validation accuracy! iteration:850 accuracy: 76.10921501706486%\n",
      "epoch 20 \n",
      "Best validation accuracy! iteration:900 accuracy: 76.79180887372013%\n",
      "epoch 21 \n",
      "epoch 22 \n",
      "Best validation accuracy! iteration:1000 accuracy: 77.9863481228669%\n",
      "epoch 23 \n",
      "epoch 24 \n",
      "epoch 25 \n",
      "epoch 26 \n",
      "epoch 27 \n",
      "epoch 28 \n",
      "Best validation accuracy! iteration:1250 accuracy: 78.66894197952219%\n",
      "epoch 29 \n",
      "epoch 30 \n",
      "Best validation accuracy! iteration:1350 accuracy: 78.839590443686%\n",
      "epoch 31 \n",
      "epoch 32 \n",
      "epoch 33 \n",
      "epoch 34 \n",
      "epoch 35 \n",
      "epoch 36 \n",
      "epoch 37 \n",
      "Best validation accuracy! iteration:1700 accuracy: 79.18088737201364%\n",
      "epoch 38 \n",
      "epoch 39 \n",
      "epoch 40 \n",
      "epoch 41 \n",
      "Best validation accuracy! iteration:1850 accuracy: 79.5221843003413%\n",
      "epoch 42 \n",
      "epoch 43 \n",
      "epoch 44 \n",
      "epoch 45 \n",
      "epoch 46 \n",
      "epoch 47 \n",
      "epoch 48 \n",
      "epoch 49 \n",
      "epoch 50 \n",
      "[  9.98891354e-01   9.27987576e-01   8.73989344e-01   6.97654366e-01\n",
      "   8.51194084e-01   9.67933178e-01   9.99927104e-01   9.97405350e-01\n",
      "   2.80377833e-04   8.97766724e-02   8.51900637e-01   5.43339908e-01\n",
      "   9.97880995e-01   4.14548479e-02   9.99809206e-01   5.46674609e-01\n",
      "   9.99955952e-01   9.98130977e-01   5.92833757e-01   9.99727428e-01\n",
      "   9.96806741e-01   9.99961436e-01   6.96848035e-01   2.61576060e-04\n",
      "   6.11735702e-01   8.12479854e-01   9.81616318e-01   4.15899121e-04\n",
      "   6.87276224e-06   3.91115900e-03   9.71560180e-01   3.31863314e-02\n",
      "   5.80844343e-01   5.95186353e-01   9.33395982e-01   9.55291808e-01\n",
      "   7.52838612e-01   3.81411868e-04   9.49848115e-01   9.89276052e-01\n",
      "   3.96096474e-03   5.50806284e-01]\n",
      "Traning ends. The best valid accuracy is 79.5221843003413. Model named lenet_1513222409.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  9.98891354e-01,   9.27987576e-01,   8.73989344e-01,\n",
       "         6.97654366e-01,   8.51194084e-01,   9.67933178e-01,\n",
       "         9.99927104e-01,   9.97405350e-01,   2.80377833e-04,\n",
       "         8.97766724e-02,   8.51900637e-01,   5.43339908e-01,\n",
       "         9.97880995e-01,   4.14548479e-02,   9.99809206e-01,\n",
       "         5.46674609e-01,   9.99955952e-01,   9.98130977e-01,\n",
       "         5.92833757e-01,   9.99727428e-01,   9.96806741e-01,\n",
       "         9.99961436e-01,   6.96848035e-01,   2.61576060e-04,\n",
       "         6.11735702e-01,   8.12479854e-01,   9.81616318e-01,\n",
       "         4.15899121e-04,   6.87276224e-06,   3.91115900e-03,\n",
       "         9.71560180e-01,   3.31863314e-02,   5.80844343e-01,\n",
       "         5.95186353e-01,   9.33395982e-01,   9.55291808e-01,\n",
       "         7.52838612e-01,   3.81411868e-04,   9.49848115e-01,\n",
       "         9.89276052e-01,   3.96096474e-03,   5.50806284e-01], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ecbm4040.neuralnets.alexnet7 import my_training\n",
    "tf.reset_default_graph()\n",
    "my_training(X_train, y_train, X_val, y_val, X_test,\n",
    "         conv_featmap=[32, 64], \n",
    "         fc_units=[84],\n",
    "         conv_kernel_size=[5, 5], \n",
    "         pooling_size=[2, 2],\n",
    "         l2_norm=0.01,\n",
    "         seed=2,\n",
    "         learning_rate=1e-2,\n",
    "         epoch=50,\n",
    "         batch_size=128,\n",
    "         verbose=False,\n",
    "         pre_trained_model=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building my LeNet. Parameters: \n",
      "conv_featmap=[15, 25]\n",
      "fc_units=[84]\n",
      "conv_kernel_size=[5, 5]\n",
      "pooling_size=[2, 2]\n",
      "l2_norm=0.01\n",
      "seed=2\n",
      "learning_rate=0.01\n",
      "number of batches for training: 100\n",
      "epoch 1 \n",
      "Best validation accuracy! iteration:100 accuracy: 51.1%\n",
      "epoch 2 \n",
      "Best validation accuracy! iteration:200 accuracy: 59.7%\n",
      "epoch 3 \n",
      "Best validation accuracy! iteration:300 accuracy: 66.4%\n",
      "epoch 4 \n",
      "Best validation accuracy! iteration:400 accuracy: 71.1%\n",
      "epoch 5 \n",
      "Best validation accuracy! iteration:500 accuracy: 73.7%\n",
      "epoch 6 \n",
      "Best validation accuracy! iteration:600 accuracy: 75.9%\n",
      "epoch 7 \n",
      "Best validation accuracy! iteration:700 accuracy: 76.5%\n",
      "epoch 8 \n",
      "Best validation accuracy! iteration:800 accuracy: 77.8%\n",
      "epoch 9 \n",
      "Best validation accuracy! iteration:900 accuracy: 78.9%\n",
      "epoch 10 \n",
      "Best validation accuracy! iteration:1000 accuracy: 80.7%\n",
      "epoch 11 \n",
      "Best validation accuracy! iteration:1100 accuracy: 80.9%\n",
      "epoch 12 \n",
      "Best validation accuracy! iteration:1200 accuracy: 81.8%\n",
      "epoch 13 \n",
      "Best validation accuracy! iteration:1300 accuracy: 82.4%\n",
      "epoch 14 \n",
      "Best validation accuracy! iteration:1400 accuracy: 83.3%\n",
      "epoch 15 \n",
      "Best validation accuracy! iteration:1500 accuracy: 83.8%\n",
      "epoch 16 \n",
      "Best validation accuracy! iteration:1600 accuracy: 84.2%\n",
      "epoch 17 \n",
      "epoch 18 \n",
      "Best validation accuracy! iteration:1800 accuracy: 84.5%\n",
      "epoch 19 \n",
      "Best validation accuracy! iteration:1900 accuracy: 84.9%\n",
      "epoch 20 \n",
      "[0 1 0 ..., 2 1 2]\n",
      "Traning ends. The best valid accuracy is 84.9. Model named lenet_1509165607.\n"
     ]
    }
   ],
   "source": [
    "from ecbm4040.neuralnets.kaggle import my_training\n",
    "tf.reset_default_graph()\n",
    "prediction = my_training(X_train, y_train, X_val, y_val, X_test, \n",
    "         conv_featmap=[15, 25], \n",
    "         fc_units=[84],\n",
    "         conv_kernel_size=[5, 5], \n",
    "         pooling_size=[2, 2],\n",
    "         l2_norm=0.01,\n",
    "         seed=2,\n",
    "         learning_rate=1e-2,\n",
    "         epoch=20,\n",
    "         batch_size=140,\n",
    "         verbose=False,\n",
    "         pre_trained_model=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate .csv file for Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The following code snippet can be used to generate your prediction .csv file.\n",
    "\n",
    "# import csv\n",
    "# with open('predicted.csv','w') as csvfile:\n",
    "#     fieldnames = ['Id','label']\n",
    "#     writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "#     writer.writeheader()    \n",
    "#     for index,l in enumerate(predicted_values_generated_by_your_model):\n",
    "#         filename = str(index)+'.png'\n",
    "#         label = str(l)\n",
    "#         writer.writerow({'Id': filename, 'label': label})\n",
    "import csv\n",
    "with open('predicted.csv','w') as csvfile:\n",
    "    fieldnames = ['Id','label']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()    \n",
    "    for i in range(3500):\n",
    "        filename = y_test[i]\n",
    "        label = str(prediction[i])\n",
    "        writer.writerow({'Id': filename, 'label': label})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
